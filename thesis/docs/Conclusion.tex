\chapter{Conclusion}
This thesis aimed to investigate the efficiency of SAT-Solvers within the context of clustered argumentation frameworks. We applied the SAT-Solver z3 to solve four different prolems. The tool we created is called \prog and is available on GitHub under the open-source MIT license. Furthermore, we designed refinements covering all implemented semantics, i.e., conflict-free, admissible and stable. These refinements are supposed to speed up the process of solving the task. Additionally, we developed two different algorithms, i.e., BFS and DFS, which operate differently and thus, one dominates the other depending on the input AFs. To be able to determine the effectiness of the refinements and which algorithm is more dominant, and under which circumstances, we ran benchmarks. In order to cover a variety of different AFs, we used three different AF generator procedures, i.e., random-based, grid-based and level-based.

In this concluding chapter, the key findings are summarized, and their implications of practicality are discussed. To wrap up this research, we also stated the efficiency of the refinements and under which circumstances one algorithm dominates the other. Finally, we will discuss the limits and issues of our implementation and how it could be improved in future works.

\paragraph{Refinements} Let us begin with discussing the impact of the refinements. As data has shown, the refinements for admissible and stable had a very small impact. We hypothesize, that by adding the refinement to the Boolean formula and almost doubling it, the SAT-Solver has to invest more computation time for a single extension. Nevertheless, due to the refinement, we are reducing the total amount of extensions which need to be calculated. We assume that these factors cancel out and the runtime is not improved significantly. However, for conflict-free the refinements contribute to a big improvement on the runtime for the most instances. There are some outliers at spurious AFs, which ban be attributed to a spurious extension, found by the DFS algorithm. For faithful AFs the refinement will always contribute to a lower runtime.


\paragraph{BFS vs DFS} Next, we will debate on the faithful/spurious check algorithm choice. The BFS approach is the algorithm with more stability, computing independently from the seed of the SAT-Solver. This is the case, because BFS does calculate all the semantic extensions before checking for spuriousness. Therefore, the order in which the extensions are calculated does not matter. BFS shows better results than DFS if the input AFs are faithful, since there are no context switches. The same holds true for AFs with very few semantics extensions. However, DFS dominates BFS in every other aspect. DFS is highly dependent on the seed of the SAT-Solver and can show spuriousness in a more efficient way, without computing all of the faithful semantic extensions. The DFS algorithm does scale good with the size of the AFs, by finding solutions even for AFs with a size of 30 arguments. Nevertheless, for faithful AFs the runtime is the same as BFS with an additional overhead of the context switches. We recommend to use the BFS algorithm for small AFs (with less arguments than 15) and if the probability of faithfulness is high. For bigger AFs (with more than 15 arguments) we recommend to use use DFS.

\paragraph{Limits and issues} We pushed \prog to the limits and analyzed the issues. First of all, the tool does not scale well with the size of the AFs. If the amount of arguments increases, the computation time of generating semantic extensions increases as well, leading to a higher runtime of all the implemented programs. Furthermore, the concretizing arguments program is not guaranteed to find a solution. This is due to the restriction of the neighbours depth (which in our implementation is $2$). By increasing the depth, s.t. the depth equals the amount of arguments the AF has, we could guarantee to find a solution, but would also aggravate the last issue: epxonential growth of the combination table at computing the concretizer list. The concretizer list defines the arguments to be concretized until faithfulness is reached and is dependent on the amount of neighbours a spurious argument has. With each new neighbour, we increase the combination table by a factor of $2$. Therefore, by increasing the depth of neighbours, we could generate a combination table taking up so much memory, that the computation is infeasible to solve.


\paragraph{Future Work} If the usage of SAT-Solvers should be preserved, the tool could be improved by substituting the current SAT-Solver (i.e., \emph{z3}) with one of the winners of the ICCMA competition (i.e., \texttt{Crustabri} or \texttt{$\mu$-Toksia}) in a future work. Due to the modular structure of the implementation, it should not be too complex. To improve the tool even further and push the current state of the art even further, the SAT-Solver usage should be reduced, since it is not as efficient as for example the ASP implementation.

The programs of \prog could also be extended in a future work. The ability to build a faithful abstract AF based on a concrete AF would be a valuable improvement.
